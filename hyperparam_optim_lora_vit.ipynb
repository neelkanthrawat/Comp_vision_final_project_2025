{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4748eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "# hyperparameter optimization rtd\n",
    "import optuna\n",
    "import wandb\n",
    "\n",
    "# os related\n",
    "import os\n",
    "\n",
    "# file handling\n",
    "\n",
    "# segmentation model\n",
    "from transformers import ViTModel, ViTImageProcessor # modules for loading the vit model\n",
    "from lora_vit import LoraVit\n",
    "from segmentation_model import SegViT\n",
    "from serial_lora_vit import SerialLoraVit\n",
    "from replora_vit import RepLoraVit\n",
    "from localised_lora_vit import LocalizedLoraVit\n",
    "from segmentation_head import CustomSegHead\n",
    "\n",
    "# dataset class\n",
    "from pet_dataset_class import PreprocessedPetDataset\n",
    "\n",
    "# dataloaders\n",
    "from create_dataloaders import get_pet_dataloaders\n",
    "\n",
    "# trainer\n",
    "from trainer import trainer\n",
    "\n",
    "# loss and metrics\n",
    "from loss_and_metrics_seg import * # idk what to import here tbh. Need to look into it\n",
    "\n",
    "# data plotting\n",
    "from data_plotting import plot_random_images_and_trimaps_2\n",
    "\n",
    "#\n",
    "from typing import Literal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fea8567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the pre-trained ViT-model (86 Mil)\n",
    "model_name = 'google/vit-base-patch16-224'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e54839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get path of image and mask files\n",
    "try:\n",
    "    base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    # __file__ is not defined (e.g. in Jupyter notebook or interactive sessions apparently), fallback to cwd\n",
    "    base_dir = os.getcwd()\n",
    "\n",
    "# Suppose your dataset is in a folder named 'data' inside the project root:\n",
    "data_dir = os.path.join(base_dir, 'data_oxford_iiit')\n",
    "\n",
    "# # Then you can define image and trimap paths relative to that\n",
    "image_folder = os.path.join(data_dir, 'resized_images')\n",
    "trimap_folder = os.path.join(data_dir, 'resized_masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc9e737e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\NEELKANTH RAWAT\\.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mneelkanth-rawat\u001b[0m (\u001b[33mnetwork-to-network\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"b47c50d9d7a54018ff9133f43a7d0d5ce08cdb1e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b12a36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BLOCKS = 12 # len(vit_pretrained.encoder.layer)\n",
    "# Deterministic LoRA layer options\n",
    "# LORA_LAYER_OPTIONS = [\n",
    "#     tuple(range(3)),                       # first 3 layers\n",
    "#     tuple(range(NUM_BLOCKS-3, NUM_BLOCKS)),# last 3 layers\n",
    "#     tuple(range(6)),                       # first 6 layers\n",
    "#     tuple(range(NUM_BLOCKS-6, NUM_BLOCKS)) # last 6 layers\n",
    "# ]\n",
    "\n",
    "def objective(\n",
    "    trial, \n",
    "    lora_type: Literal[\"lora\", \"serial_lora\", \"replora\", \"localised_lora\"] = \"lora\", \n",
    "    optimize_for: Literal[\"loss\", \"dice\", \"iou\"] = \"loss\",\n",
    "    separate_lr = False,\n",
    "    want_backbone_frozen_initially=False\n",
    "):\n",
    "    # Sample hyperparameters\n",
    "    lora_rank = trial.suggest_categorical(\"lora_rank\", [4, 8, 16])\n",
    "    lora_alpha = trial.suggest_categorical(\"lora_alpha\", [4, 8, 16, 32])\n",
    "    if separate_lr:\n",
    "        lr_backbone = trial.suggest_float(\"lr_vit_backbone\", 1e-5, 1e-3, log=True)\n",
    "        lr_head = trial.suggest_float(\"lr_seg_head\", 1e-4, 1e-3, log=True)\n",
    "    else:\n",
    "        lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    #weight_decay = trial.suggest_float(\"weight_decay\", 1e-4, 1e-2, log=True)# L2 decay\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adamw\", \"adam\"]) \n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [24, 32])\n",
    "    #dropout_rate = trial.suggest_categorical(\"dropout_rate\", [0.0, 0.15])\n",
    "    #use_bn = trial.suggest_categorical(\"use_bn\", [True, False])\n",
    "    if lora_type==\"localised_lora\":\n",
    "        # add rblock and num_blocks as parameters here\n",
    "        r_block = trial.suggest_categorical(\"r_block\",[2,4,8,16])\n",
    "        num_blocks = trial.suggest_categorical(\"num_blocks\", [2,4,8,16])\n",
    "    \n",
    "    if want_backbone_frozen_initially:\n",
    "        freeze_epochs=trial.suggest_categorical(\"freeze_epochs\",[2,5,10])\n",
    "\n",
    "    # W&B SETUP\n",
    "    # #W&B config setup\n",
    "    wandb_config = {\n",
    "        \"lora_type\": lora_type,\n",
    "        \"lora_rank\": lora_rank,\n",
    "        \"lora_alpha\": lora_alpha,\n",
    "        \"optimizer\": optimizer_name,\n",
    "        \"batch_size\": batch_size,\n",
    "        \"separate_lr\": separate_lr,\n",
    "        \"want_backbone_frozen_initially\": want_backbone_frozen_initially,\n",
    "    }\n",
    "    if lora_type == \"localised_lora\":\n",
    "        wandb_config.update({\n",
    "            \"r_block\": r_block,\n",
    "            \"num_blocks_per_row\": num_blocks\n",
    "        })\n",
    "    if want_backbone_frozen_initially: # I am not quite sure if we want to do it orn\n",
    "        wandb_config.update({\n",
    "            \"freeze_epochs\": freeze_epochs\n",
    "        })\n",
    "    if separate_lr:\n",
    "        wandb_config.update({\n",
    "            \"lr_vit_backbone\": lr_backbone,\n",
    "            \"lr_seg_head\": lr_head\n",
    "        })\n",
    "    else:\n",
    "        wandb_config.update({\"lr\": lr})\n",
    "    ## Initialize W&B\n",
    "    wandb.init(\n",
    "        project=\"Lora_vit_segmentation\",\n",
    "        config=wandb_config,\n",
    "        reinit='finish_previous'\n",
    "    )\n",
    "\n",
    "    # some other parameters which I have decided to keep fixed\n",
    "    weight_decay=0.0005\n",
    "    use_bn=True\n",
    "    dropout_rate=0.1\n",
    "    num_epoch=50\n",
    "    if want_backbone_frozen_initially: # I am not quite sure if we want to do it orn\n",
    "        wandb_config.update({\n",
    "            \"freeze_epochs\": freeze_epochs\n",
    "        })\n",
    "\n",
    "    # Create model with current trial's hyperparameters\n",
    "    Vit_pretrained = ViTModel.from_pretrained(model_name)\n",
    "    if lora_type == \"lora\":\n",
    "        lora_vit_base = LoraVit(vit_model=Vit_pretrained, r=lora_rank, alpha=lora_alpha)\n",
    "    elif lora_type == \"serial_lora\":\n",
    "        lora_vit_base = SerialLoraVit(vit_model=Vit_pretrained, r=lora_rank)\n",
    "    elif lora_type == \"replora\":\n",
    "        lora_vit_base = RepLoraVit(vit_model=Vit_pretrained, r=lora_rank, alpha=lora_alpha)\n",
    "    elif lora_type == \"localised_lora\":\n",
    "        # i still need to calculate r_block and num_blocks properly\n",
    "        lora_vit_base = LocalizedLoraVit(vit_model=Vit_pretrained,\n",
    "                                        r_block=r_block,\n",
    "                                        alpha=lora_alpha,\n",
    "                                        num_blocks_per_row=num_blocks)\n",
    "    \n",
    "    seg_head = CustomSegHead(hidden_dim=768, num_classes=3,\n",
    "                            patch_size=16,image_size=224,\n",
    "                            dropout_rate=dropout_rate, \n",
    "                            use_bn=use_bn)\n",
    "    \n",
    "    vit_seg_model = SegViT(vit_model=lora_vit_base,\n",
    "                        image_size=224, patch_size=16,\n",
    "                        dim=768, n_classes=3,\n",
    "                        head=seg_head)\n",
    "\n",
    "    # Create dataloaders with sampled batch size \n",
    "    train_dl, val_dl, _ = get_pet_dataloaders(\n",
    "        image_folder=image_folder,\n",
    "        mask_folder=trimap_folder,\n",
    "        DatasetClass=PreprocessedPetDataset,\n",
    "        all_data=False,\n",
    "        num_datapoints=25,\n",
    "        val_ratio=0.2,\n",
    "        test_ratio=0.1,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Setup optimizer\n",
    "    optimizer_cls = torch.optim.AdamW if optimizer_name == \"adamw\" else torch.optim.Adam\n",
    "    if separate_lr:\n",
    "        optimizer = optimizer_cls([\n",
    "            {\"params\": vit_seg_model.backbone_parameters, \"lr\": lr_backbone},\n",
    "            {\"params\": vit_seg_model.head_parameters, \"lr\": lr_head},\n",
    "        ], weight_decay=weight_decay)\n",
    "    else:\n",
    "        optimizer = optimizer_cls(vit_seg_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Trainer\n",
    "    trainer_input_params = {\n",
    "        \"model\": vit_seg_model,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"criterion\": log_cosh_dice_loss,\n",
    "        \"num_epoch\": num_epoch,\n",
    "        \"dataloaders\": {\"train\": train_dl, \"val\": val_dl},\n",
    "        \"use_trap_scheduler\":True,\n",
    "        \"device\": \"cpu\",\n",
    "        \"criterion_kwargs\": {\"num_classes\": 3, \"epsilon\": 1e-6},\n",
    "        \"want_backbone_frozen_initially\":want_backbone_frozen_initially,\n",
    "        \"freeze_epochs\":freeze_epochs # I have fixed it to 3\n",
    "    }\n",
    "\n",
    "    trainer_seg_model = trainer(**trainer_input_params)\n",
    "    # trainer_seg_model.train() # I had to comment it because for W&B, to log and plot curves in real time, I need to create loopwise functions here\n",
    "    \n",
    "    # Training loop with W&B logging\n",
    "    for epoch in range(trainer_seg_model.num_epoch):\n",
    "        #  train and validation step for each epoch\n",
    "        avg_train_loss = trainer_seg_model.train_epoch(epoch)\n",
    "        avg_val_loss, avg_val_dice, avg_val_iou = trainer_seg_model.val_epoch()# unpack all three values returned by val_epoch\n",
    "\n",
    "        # accumulate losses\n",
    "        trainer_seg_model.train_error_epoch_list.append(avg_train_loss)\n",
    "        trainer_seg_model.val_error_epoch_list.append(avg_val_loss)\n",
    "        # accumulate metrics\n",
    "        trainer_seg_model.val_dice_epoch_list.append(avg_val_dice)\n",
    "        trainer_seg_model.val_iou_epoch_list.append(avg_val_iou)\n",
    "\n",
    "        # log to W&B\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"val_dice\": avg_val_dice,\n",
    "            \"val_iou\": avg_val_iou,\n",
    "            \"lr\": optimizer.param_groups[0][\"lr\"]\n",
    "        })\n",
    "\n",
    "\n",
    "    # after we train(), in the class trainer_seg_model, validation loss and validation metrics are stored for each epoch as class attributes\n",
    "    if optimize_for == \"loss\":\n",
    "        result = trainer_seg_model.val_error_epoch_list[-1]\n",
    "    elif optimize_for == \"dice\":\n",
    "        result = trainer_seg_model.val_dice_epoch_list[-1]\n",
    "    elif optimize_for == \"iou\":\n",
    "        result = trainer_seg_model.val_iou_epoch_list[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimize_for='{optimize_for}'\")\n",
    "\n",
    "    # finish W&B run\n",
    "    wandb.finish() \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc4cb49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize for loss\n",
    "N_TRIALS=2\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(lambda trial: objective(trial, optimize_for=\"loss\"), n_trials=N_TRIALS)\n",
    "# Print best hyperparameters\n",
    "print(study.best_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3493a3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lora_rank': 16, 'lora_alpha': 32, 'lora_layers': (0, 1, 2), 'lr': 7.148500193053507e-05, 'weight_decay': 0.004557645356079437, 'optimizer': 'adam', 'batch_size': 16, 'dropout_rate': 0.0, 'use_bn': False}\n"
     ]
    }
   ],
   "source": [
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d9b07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize for dice\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(lambda trial: objective(trial, optimize_for=\"dice\"), n_trials=N_TRIALS)\n",
    "# Print best hyperparameters\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1841ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize for IoU\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(lambda trial: objective(trial, optimize_for=\"iou\"), \n",
    "            n_trials=N_TRIALS)\n",
    "# Print best hyperparameters\n",
    "print(study.best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
