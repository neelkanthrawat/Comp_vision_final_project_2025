{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4748eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "# hyperparameter optimization rtd\n",
    "import optuna\n",
    "import wandb\n",
    "\n",
    "# os related\n",
    "import os\n",
    "\n",
    "# file handling\n",
    "\n",
    "# segmentation model\n",
    "from transformers import ViTModel, ViTImageProcessor # modules for loading the vit model\n",
    "from lora_vit import LoraVit\n",
    "from segmentation_model import SegViT\n",
    "from segmentation_head import CustomSegHead\n",
    "\n",
    "# dataset class\n",
    "from pet_dataset_class import PreprocessedPetDataset\n",
    "\n",
    "# dataloaders\n",
    "from create_dataloaders import get_pet_dataloaders\n",
    "\n",
    "# trainer\n",
    "from trainer import trainer\n",
    "\n",
    "# loss and metrics\n",
    "from loss_and_metrics_seg import * # idk what to import here tbh. Need to look into it\n",
    "\n",
    "# data plotting\n",
    "from data_plotting import plot_random_images_and_trimaps_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fea8567f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## load the pre-trained ViT-model (86 Mil)\n",
    "model_name = 'google/vit-base-patch16-224'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e54839a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get path of image and mask files\n",
    "try:\n",
    "    base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    # __file__ is not defined (e.g. in Jupyter notebook or interactive sessions apparently), fallback to cwd\n",
    "    base_dir = os.getcwd()\n",
    "\n",
    "# Suppose your dataset is in a folder named 'data' inside the project root:\n",
    "data_dir = os.path.join(base_dir, 'data_oxford_iiit')\n",
    "\n",
    "# # Then you can define image and trimap paths relative to that\n",
    "image_folder = os.path.join(data_dir, 'resized_images')\n",
    "trimap_folder = os.path.join(data_dir, 'resized_masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc9e737e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\NEELKANTH RAWAT\\.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mneelkanth-rawat\u001b[0m (\u001b[33mnetwork-to-network\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login(key=\"b47c50d9d7a54018ff9133f43a7d0d5ce08cdb1e\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b12a36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_BLOCKS = 12 # len(vit_pretrained.encoder.layer)\n",
    "# Deterministic LoRA layer options\n",
    "LORA_LAYER_OPTIONS = [\n",
    "    tuple(range(3)),                       # first 3 layers\n",
    "    tuple(range(NUM_BLOCKS-3, NUM_BLOCKS)),# last 3 layers\n",
    "    tuple(range(6)),                       # first 6 layers\n",
    "    tuple(range(NUM_BLOCKS-6, NUM_BLOCKS)) # last 6 layers\n",
    "]\n",
    "\n",
    "def objective(trial, optimize_for=\"loss\"):\n",
    "    # Sample hyperparameters\n",
    "    lora_rank = trial.suggest_categorical(\"lora_rank\", [4, 8, 16])\n",
    "    lora_alpha = trial.suggest_categorical(\"lora_alpha\", [4, 8, 16, 32])\n",
    "    \n",
    "    lora_layers = trial.suggest_categorical(\"lora_layers\", LORA_LAYER_OPTIONS)\n",
    "    \n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-3, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-4, 1e-2, log=True)# L2 decay\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"adamw\", \"adam\"]) \n",
    "    \n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16,24])#, 24, 32])\n",
    "    dropout_rate = trial.suggest_categorical(\"dropout_rate\", [0.0, 0.15])\n",
    "    use_bn = trial.suggest_categorical(\"use_bn\", [True, False])\n",
    "\n",
    "    # W&B setup\n",
    "    wandb.init(\n",
    "        project=\"Lora_vit_segmentation\",\n",
    "        config={\n",
    "            \"lora_rank\": lora_rank,\n",
    "            \"lora_alpha\": lora_alpha,\n",
    "            \"lora_layers\": lora_layers,\n",
    "            \"lr\": lr,\n",
    "            \"weight_decay\": weight_decay,\n",
    "            \"optimizer\": optimizer_name,\n",
    "            \"batch_size\": batch_size,\n",
    "            \"dropout_rate\": dropout_rate,\n",
    "            \"use_bn\": use_bn,\n",
    "            \"num_epoch\": 3\n",
    "        },\n",
    "        reinit='finish_previous' # This ensures that a new W&B run is created for each Optuna trial.\n",
    "    )\n",
    "\n",
    "    # Create model with current trial's hyperparameters\n",
    "    #image_processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "    Vit_pretrained = ViTModel.from_pretrained(model_name)\n",
    "    lora_vit_base = LoraVit(vit_model=Vit_pretrained,# this vit_pretrained is defined globally (in cell number 3)\n",
    "                            r=lora_rank, alpha=lora_alpha,\n",
    "                            lora_layers=lora_layers)\n",
    "    \n",
    "    seg_head = CustomSegHead(hidden_dim=768, num_classes=3,                 patch_size=16,image_size=224,\n",
    "                            dropout_rate=dropout_rate, use_bn=use_bn)\n",
    "    \n",
    "    vit_seg_model = SegViT(vit_model=lora_vit_base,\n",
    "                        image_size=224, patch_size=16,\n",
    "                        dim=768, n_classes=3,\n",
    "                        head=seg_head)\n",
    "\n",
    "    # Create dataloaders with sampled batch size \n",
    "    train_dl, val_dl, _ = get_pet_dataloaders(\n",
    "        image_folder=image_folder,\n",
    "        mask_folder=trimap_folder,\n",
    "        DatasetClass=PreprocessedPetDataset,\n",
    "        all_data=False,\n",
    "        num_datapoints=25,\n",
    "        val_ratio=0.2,\n",
    "        test_ratio=0.1,\n",
    "        batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    # Setup optimizer\n",
    "    optimizer_cls = torch.optim.AdamW if optimizer_name == \"adamw\" else torch.optim.Adam\n",
    "    optimizer = optimizer_cls(vit_seg_model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # Trainer\n",
    "    trainer_input_params = {\n",
    "        \"model\": vit_seg_model,\n",
    "        \"optimizer\": optimizer,\n",
    "        \"lr\": lr,\n",
    "        \"criterion\": log_cosh_dice_loss,\n",
    "        \"num_epoch\": 3,\n",
    "        \"dataloaders\": {\"train\": train_dl, \"val\": val_dl},\n",
    "        \"use_trap_scheduler\":True,\n",
    "        \"device\": \"cpu\",\n",
    "        \"criterion_kwargs\": {\"num_classes\": 3, \"epsilon\": 1e-6}\n",
    "    }\n",
    "\n",
    "    trainer_seg_model = trainer(**trainer_input_params)\n",
    "    # trainer_seg_model.train() # I had to comment it because for W&B, to log and plot curves in real time, I need to create loopwise functions here\n",
    "    \n",
    "    # Training loop with W&B logging\n",
    "    for epoch in range(trainer_seg_model.num_epoch):\n",
    "        #  train and validation step for each epoch\n",
    "        avg_train_loss = trainer_seg_model.train_epoch(epoch)\n",
    "        avg_val_loss, avg_val_dice, avg_val_iou = trainer_seg_model.val_epoch()# unpack all three values returned by val_epoch\n",
    "\n",
    "        # accumulate losses\n",
    "        trainer_seg_model.train_error_epoch_list.append(avg_train_loss)\n",
    "        trainer_seg_model.val_error_epoch_list.append(avg_val_loss)\n",
    "        # accumulate metrics\n",
    "        trainer_seg_model.val_dice_epoch_list.append(avg_val_dice)\n",
    "        trainer_seg_model.val_iou_epoch_list.append(avg_val_iou)\n",
    "\n",
    "        # log to W&B\n",
    "        wandb.log({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": avg_train_loss,\n",
    "            \"val_loss\": avg_val_loss,\n",
    "            \"val_dice\": avg_val_dice,\n",
    "            \"val_iou\": avg_val_iou,\n",
    "            \"lr\": optimizer.param_groups[0][\"lr\"]\n",
    "        })\n",
    "\n",
    "\n",
    "    # after we train(), in the class trainer_seg_model, validation loss and validation metrics are stored for each epoch as class attributes\n",
    "    if optimize_for == \"loss\":\n",
    "        result = trainer_seg_model.val_error_epoch_list[-1]\n",
    "    elif optimize_for == \"dice\":\n",
    "        result = trainer_seg_model.val_dice_epoch_list[-1]\n",
    "    elif optimize_for == \"iou\":\n",
    "        result = trainer_seg_model.val_iou_epoch_list[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown optimize_for='{optimize_for}'\")\n",
    "\n",
    "    # finish W&B run\n",
    "    wandb.finish() \n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fc4cb49",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-19 15:55:59,799] A new study created in memory with name: no-name-3253dfb8-6e95-498e-b829-3556a665a47e\n",
      "c:\\Users\\NEELKANTH RAWAT\\.conda\\envs\\vit\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (0, 1, 2) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\NEELKANTH RAWAT\\.conda\\envs\\vit\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (9, 10, 11) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\NEELKANTH RAWAT\\.conda\\envs\\vit\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (0, 1, 2, 3, 4, 5) which is of type tuple.\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\NEELKANTH RAWAT\\.conda\\envs\\vit\\Lib\\site-packages\\optuna\\distributions.py:518: UserWarning: Choices for a categorical distribution should be a tuple of None, bool, int, float and str for persistent storage but contains (6, 7, 8, 9, 10, 11) which is of type tuple.\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for wandb.init()..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Desktop\\master_scientific_computing\\fourth_semester\\Comp_vision_final_project_2025\\wandb\\run-20250819_155559-uk007mtp</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/network-to-network/Lora_vit_segmentation/runs/uk007mtp' target=\"_blank\">graceful-donkey-26</a></strong> to <a href='https://wandb.ai/network-to-network/Lora_vit_segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/network-to-network/Lora_vit_segmentation' target=\"_blank\">https://wandb.ai/network-to-network/Lora_vit_segmentation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/network-to-network/Lora_vit_segmentation/runs/uk007mtp' target=\"_blank\">https://wandb.ai/network-to-network/Lora_vit_segmentation/runs/uk007mtp</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using only 25 datapoints out of 7390 total files.\n",
      "train_size: 18, val_size: 5 and test_size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Training:   0%|          | 0/2 [00:00<?, ?it/s]c:\\Users\\NEELKANTH RAWAT\\.conda\\envs\\vit\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:198: UserWarning: The epoch parameter in `scheduler.step()` was not necessary and is being deprecated where possible. Please use `scheduler.step()` to step the scheduler. During the deprecation, if epoch is different from None, the closed form is used instead of the new chainable form, where available. Please open an issue if you are unable to replicate your use case: https://github.com/pytorch/pytorch/issues/new/choose.\n",
      "  warnings.warn(EPOCH_DEPRECATION_WARNING, UserWarning)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>lr</td><td>▁▁▁</td></tr><tr><td>train_loss</td><td>█▃▁</td></tr><tr><td>val_dice</td><td>▁▅█</td></tr><tr><td>val_iou</td><td>▁▅█</td></tr><tr><td>val_loss</td><td>█▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>lr</td><td>7e-05</td></tr><tr><td>train_loss</td><td>0.15986</td></tr><tr><td>val_dice</td><td>0.37296</td></tr><tr><td>val_iou</td><td>0.24268</td></tr><tr><td>val_loss</td><td>0.18492</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">graceful-donkey-26</strong> at: <a href='https://wandb.ai/network-to-network/Lora_vit_segmentation/runs/uk007mtp' target=\"_blank\">https://wandb.ai/network-to-network/Lora_vit_segmentation/runs/uk007mtp</a><br> View project at: <a href='https://wandb.ai/network-to-network/Lora_vit_segmentation' target=\"_blank\">https://wandb.ai/network-to-network/Lora_vit_segmentation</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250819_155559-uk007mtp\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-19 15:59:57,244] Trial 0 finished with value: 0.18491728603839874 and parameters: {'lora_rank': 16, 'lora_alpha': 32, 'lora_layers': (0, 1, 2), 'lr': 7.148500193053507e-05, 'weight_decay': 0.004557645356079437, 'optimizer': 'adam', 'batch_size': 16, 'dropout_rate': 0.0, 'use_bn': False}. Best is trial 0 with value: 0.18491728603839874.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Desktop\\master_scientific_computing\\fourth_semester\\Comp_vision_final_project_2025\\wandb\\run-20250819_155957-cffxzvw0</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/network-to-network/Lora_vit_segmentation/runs/cffxzvw0' target=\"_blank\">decent-frost-27</a></strong> to <a href='https://wandb.ai/network-to-network/Lora_vit_segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/network-to-network/Lora_vit_segmentation' target=\"_blank\">https://wandb.ai/network-to-network/Lora_vit_segmentation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/network-to-network/Lora_vit_segmentation/runs/cffxzvw0' target=\"_blank\">https://wandb.ai/network-to-network/Lora_vit_segmentation/runs/cffxzvw0</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using only 25 datapoints out of 7390 total files.\n",
      "train_size: 18, val_size: 5 and test_size: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▅█</td></tr><tr><td>lr</td><td>▁▁▁</td></tr><tr><td>train_loss</td><td>█▄▁</td></tr><tr><td>val_dice</td><td>▁▅█</td></tr><tr><td>val_iou</td><td>▁▅█</td></tr><tr><td>val_loss</td><td>█▄▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>3</td></tr><tr><td>lr</td><td>3e-05</td></tr><tr><td>train_loss</td><td>0.19302</td></tr><tr><td>val_dice</td><td>0.33663</td></tr><tr><td>val_iou</td><td>0.20878</td></tr><tr><td>val_loss</td><td>0.20556</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">decent-frost-27</strong> at: <a href='https://wandb.ai/network-to-network/Lora_vit_segmentation/runs/cffxzvw0' target=\"_blank\">https://wandb.ai/network-to-network/Lora_vit_segmentation/runs/cffxzvw0</a><br> View project at: <a href='https://wandb.ai/network-to-network/Lora_vit_segmentation' target=\"_blank\">https://wandb.ai/network-to-network/Lora_vit_segmentation</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20250819_155957-cffxzvw0\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-08-19 16:03:16,471] Trial 1 finished with value: 0.20556245744228363 and parameters: {'lora_rank': 16, 'lora_alpha': 4, 'lora_layers': (9, 10, 11), 'lr': 3.127560309360304e-05, 'weight_decay': 0.0005006394791299488, 'optimizer': 'adamw', 'batch_size': 16, 'dropout_rate': 0.15, 'use_bn': False}. Best is trial 0 with value: 0.18491728603839874.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lora_rank': 16, 'lora_alpha': 32, 'lora_layers': (0, 1, 2), 'lr': 7.148500193053507e-05, 'weight_decay': 0.004557645356079437, 'optimizer': 'adam', 'batch_size': 16, 'dropout_rate': 0.0, 'use_bn': False}\n"
     ]
    }
   ],
   "source": [
    "# Optimize for loss\n",
    "N_TRIALS=2\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(lambda trial: objective(trial, optimize_for=\"loss\"), n_trials=N_TRIALS)\n",
    "# Print best hyperparameters\n",
    "print(study.best_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3493a3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lora_rank': 16, 'lora_alpha': 32, 'lora_layers': (0, 1, 2), 'lr': 7.148500193053507e-05, 'weight_decay': 0.004557645356079437, 'optimizer': 'adam', 'batch_size': 16, 'dropout_rate': 0.0, 'use_bn': False}\n"
     ]
    }
   ],
   "source": [
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d9b07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize for dice\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(lambda trial: objective(trial, optimize_for=\"dice\"), n_trials=N_TRIALS)\n",
    "# Print best hyperparameters\n",
    "print(study.best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1841ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize for IoU\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(lambda trial: objective(trial, optimize_for=\"iou\"), \n",
    "            n_trials=N_TRIALS)\n",
    "# Print best hyperparameters\n",
    "print(study.best_params)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
