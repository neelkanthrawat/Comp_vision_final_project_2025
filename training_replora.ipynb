{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aaae003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "# hyperparameter optimization rtd\n",
    "import optuna\n",
    "import wandb\n",
    "\n",
    "# os related\n",
    "import os\n",
    "\n",
    "# file handling\n",
    "\n",
    "# segmentation model\n",
    "from replora_vit import RepLoraVit\n",
    "from segmentation_model import SegViT\n",
    "from segmentation_head import CustomSegHead\n",
    "\n",
    "# dataset class\n",
    "from pet_dataset_class import PreprocessedPetDataset\n",
    "\n",
    "# dataloaders\n",
    "from create_dataloaders import get_pet_dataloaders\n",
    "\n",
    "# trainer\n",
    "from trainer import trainer\n",
    "\n",
    "# loss and metrics\n",
    "from loss_and_metrics_seg import * # idk what to import here tbh. Need to look into it\n",
    "\n",
    "# data plotting\n",
    "from data_plotting import plot_random_images_and_trimaps_2\n",
    "\n",
    "# modules for loading the vit model\n",
    "from transformers import ViTModel, ViTImageProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## load the pre-trained ViT-model (86 Mil)\n",
    "model_name = 'google/vit-base-patch16-224'\n",
    "\n",
    "image_processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "VIT_PRETRAINED = ViTModel.from_pretrained(model_name)\n",
    "\n",
    "# instantiating the lora vit based backbone model\n",
    "lora_vit_base = RepLoraVit(vit_model=VIT_PRETRAINED,\n",
    "                        r=4,alpha = 16, lora_layers = [1,2,3,4])\n",
    "\n",
    "\n",
    "# # instantiate the custom segmentation head\n",
    "# check_seg_head_model = CustomSegHead(hidden_dim=768, num_classes=3, patch_size=16, image_size=224) #  do not need to do that, the SegViT model will do it automatically\n",
    "\n",
    "# instantiate the segmentation model\n",
    "vit_seg_model = SegViT(vit_model=lora_vit_base,image_size=224, patch_size=16\n",
    "                    , dim= 768,\n",
    "                    n_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf8f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I just noticed this warning: \"Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a8778521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using only 80 datapoints out of 7390 total files.\n",
      "train_size: 64, val_size: 8 and test_size: 8\n"
     ]
    }
   ],
   "source": [
    "# get path of image and mask files\n",
    "try:\n",
    "    base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    # __file__ is not defined (e.g. in Jupyter notebook or interactive sessions apparently), fallback to cwd\n",
    "    base_dir = os.getcwd()\n",
    "\n",
    "# Suppose your dataset is in a folder named 'data' inside the project root:\n",
    "data_dir = os.path.join(base_dir, 'data_oxford_iiit')\n",
    "\n",
    "# # Then you can define image and trimap paths relative to that\n",
    "image_folder = os.path.join(data_dir, 'resized_images')\n",
    "trimap_folder = os.path.join(data_dir, 'resized_masks')\n",
    "\n",
    "# create dataloaders\n",
    "train_dl, val_dl, test_dl = get_pet_dataloaders(\n",
    "    image_folder=image_folder,\n",
    "    mask_folder=trimap_folder,\n",
    "    DatasetClass=PreprocessedPetDataset,\n",
    "    all_data=False,\n",
    "    num_datapoints=80, # right now I am just testing whether the things would work or not. I will intentionally overtrain it.\n",
    "    batch_size=32\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the input parameters for the trainer\n",
    "trainer_input_params = {\n",
    "    \"model\": vit_seg_model , \n",
    "    \"optimizer\": torch.optim.Adam(params=vit_seg_model.parameters(), lr=5e-5),\n",
    "#     torch.optim.Adam([\n",
    "#     {\"params\": vit_seg_model.backbone_parameters, \"lr\": 1e-5},\n",
    "#     {\"params\": vit_seg_model.head_parameters, \"lr\": 1e-4},\n",
    "# ]),\n",
    "    #\"lr\": 1e-4, # I do not think we need to have this learning rate seperately. need to fix it here :)\n",
    "    \"criterion\": log_cosh_dice_loss,  # or log_cosh_dice_loss, whichever you want to use\n",
    "    \"num_epoch\": 8,\n",
    "    \"dataloaders\": {\n",
    "        \"train\": train_dl,  # replace with your actual DataLoader\n",
    "        \"val\": val_dl       # replace with your actual DataLoader\n",
    "    },\n",
    "    \"use_trap_scheduler\": False,             # or your scheduler instance if you use one\n",
    "    \"device\": \"cpu\",#\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    #\"model_kwargs\": {},            # add any extra forward() kwargs if needed\n",
    "    \"criterion_kwargs\": {\n",
    "        \"num_classes\": 3,\n",
    "        \"epsilon\": 1e-6,\n",
    "        # \"return_metrics\": False    # usually False for training, True for validation if you want metrics ## WE DO NOT NEED THIS HERE\n",
    "    }\n",
    "    #,\"want_backbone_frozen_initially\": False,\n",
    "    #\"freeze_epochs\":2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d7b9c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the trainer:\n",
    "trainer_seg_model = trainer(**trainer_input_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce925faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_seg_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615a87ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yeah I had a feeling that we might get some error messahe here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
