{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca8e79f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "347a3e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel, ViTImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "551a5e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## load the pre-trained ViT-model (86 Mil)\n",
    "model_name = 'google/vit-base-patch16-224'\n",
    "\n",
    "# \n",
    "image_processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "model = ViTModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7fa7b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 86,389,248\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "# 86 million model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a89a5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTModel(\n",
       "  (embeddings): ViTEmbeddings(\n",
       "    (patch_embeddings): ViTPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): ViTEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x ViTLayer(\n",
       "        (attention): ViTAttention(\n",
       "          (attention): ViTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (pooler): ViTPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d759253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ec0fe51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "768"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder.layer[0].attention.attention.query.in_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cbef6a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block_i.attention.attention.query: Linear(in_features=768, out_features=768, bias=True)\n",
      "block_i.attention.attention.value: Linear(in_features=768, out_features=768, bias=True)\n"
     ]
    }
   ],
   "source": [
    "for layer_i, block_i in enumerate(model.encoder.layer):\n",
    "    print(\"block_i.attention.attention.query:\",block_i.attention.attention.query)\n",
    "    print(\"block_i.attention.attention.value:\",block_i.attention.attention.value)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8fcddc",
   "metadata": {},
   "source": [
    "This is the architecture of the vit model\n",
    "```\n",
    "model.encoder.layer[0]\n",
    "└── attention\n",
    "    └── attention\n",
    "        ├── query: nn.Linear\n",
    "        ├── key:   nn.Linear\n",
    "        └── value: nn.Linear\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41c2ba2",
   "metadata": {},
   "source": [
    "# Things to do:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0053d7",
   "metadata": {},
   "source": [
    "## Stage 0. We need to come up with some framework of the project and what exactly are we going to do\n",
    "\n",
    "## Stage 1. Set up a Transformer based segmentation model using ViT+LoRA\n",
    "0. Understand and play around with the models\n",
    "1. Load the pre-trained model\n",
    "    1.If required, we might have to switch to timm\n",
    "2. Define the class for LoRA:\n",
    "    2.1: Either can be set up using `peft` library\n",
    "    2.2: Build a custom LoRA module using Pytorch.\n",
    "3. Apply LoRA to attention layers\n",
    "4. Define (code) and add a segmentation head\n",
    "    1. Simple MLP or some more complicated architecture? We need to look into it.\n",
    "5. Training Set-Up\n",
    "    5.1. Loss for the segmentation task\n",
    "    5.2 optimizer\n",
    "6. Training:\n",
    "    6.1 typical LoRA rank (r): 4 or 8 -  a good balance for fine tuning\n",
    "    6.2 How many parameters to freeze\n",
    "\n",
    "Obtain some simple acceptable results for this. \n",
    "\n",
    "## Stage 2. Try  1 more different versions of LoRA for the same task:\n",
    "0. Serial LoRA for the ViT (recent paper)\n",
    "1. Other  DoRA, etc. (check that review paper for other version)\n",
    "\n",
    "## Stage 3: Try 1 other different fine tuning strategy, maybe some other adapter based approach/ IA3 etc. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76a7a17",
   "metadata": {},
   "source": [
    "# LoRA implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5031d8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import math\n",
    "from safetensors.torch import save_file, load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46c95e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "### implement the LoRAlayer\n",
    "class LoraLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements the LoRA layer\n",
    "    wt_linear: Weight (which would be left frozen)\n",
    "    A,B: Lower matrices which constitute delta W\n",
    "    rank_lora: Rank of A and B matrices\n",
    "    alpha: some weighing factor\n",
    "    \"\"\"\n",
    "    def __init__(self, wt: nn.Module, A: nn.Module, B: nn.Module, rank_lora: int, alpha: int):\n",
    "        super().__init__()\n",
    "        self.wt = wt\n",
    "        self.A, self.B = A, B\n",
    "        self.rank = rank_lora\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x=self.wt(x) + (self.alpha / self.rank) * self.B(self.A(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "150ae4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## implement the LoRA VIT\n",
    "### ? Things to check ?\n",
    "## ? vit_model.transformer.blocks --> replaced with model.encoder.layer\n",
    "## ? block.attn.proj_q/p ---> replace with block.attention.attention.query/value.\n",
    "## ? vit_model.parameters() <--> Stays the same\n",
    "\n",
    "class LoraVit(nn.Module):\n",
    "    \"\"\" \n",
    "    This class is to introduce LoRA layer to the model.\n",
    "    vit_model: pre-trained vit model\n",
    "    r: rank\n",
    "    alpha: scaling strength for lora\n",
    "    lora_layers: Layers we want to apply lora to\n",
    "    \"\"\"\n",
    "    def __init__(self, vit_model, r:int, alpha:int, lora_layers=None):\n",
    "        super().__init__()\n",
    "\n",
    "        assert r>0, \"r (rank of lora matrices) must be >0\"\n",
    "        assert alpha>0 , \"alpha >0\"\n",
    "\n",
    "        if lora_layers:\n",
    "            self.lora_layers = lora_layers\n",
    "        else: ## apply lora to all\n",
    "            ## ? here I need to see how will I check the number of transformer blocks\n",
    "            self.lora_layers = list(range(len(vit_model.encoder.layer)))\n",
    "        \n",
    "        # Dimension of the input vector to the transformer\n",
    "        dim = vit_model.encoder.layer[0].attention.attention.query.in_features \n",
    "        print(f\"dim is: {dim}\")\n",
    "        \n",
    "        # freeze the parameters\n",
    "        ## ? How can we invoke paramters in the vit_model\n",
    "        for param in vit_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        ## for storing the lora parameters\n",
    "        self.list_q_As, self.list_q_Bs = [], []\n",
    "        self.list_v_As, self.list_v_Bs = [], []\n",
    "\n",
    "        # replace the normal q and V with LoRA layers\n",
    "        for layer_i, block_i in enumerate(vit_model.encoder.layer):\n",
    "            if layer_i not in self.lora_layers:\n",
    "                continue # (next iteration)\n",
    "            w_q_linear = block_i.attention.attention.query\n",
    "            w_v_linear = block_i.attention.attention.value\n",
    "            # Q and V layers' weights\n",
    "\n",
    "            ## do I need to initialise weights here? or should I do it after this loop?\n",
    "            a_linear_q = nn.Linear(dim, r, bias=False)\n",
    "            b_linear_q = nn.Linear(r, dim, bias=False)\n",
    "            a_linear_v = nn.Linear(dim, r, bias=False)\n",
    "            b_linear_v = nn.Linear(r, dim, bias=False)\n",
    "\n",
    "            # Append lora params to the list\n",
    "            self.list_q_As.append(a_linear_q); self.list_q_Bs.append(b_linear_q)\n",
    "            self.list_v_As.append(a_linear_v); self.list_v_Bs.append(b_linear_v) \n",
    "\n",
    "            # replace with LoRA layer\n",
    "            # block_i.attn.proj_q = LoraLayer(w_q_linear, a_linear_q, b_linear_q, r, alpha)\n",
    "            block_i.attention.attention.query = LoraLayer(w_q_linear, a_linear_q, b_linear_q, r, alpha)\n",
    "            # block_i.attn.proj_v = LoraLayer(w_v_linear, a_linear_v, b_linear_v, r, alpha)\n",
    "            block_i.attention.attention.value =  LoraLayer(w_v_linear, a_linear_v, b_linear_v, r, alpha)\n",
    "\n",
    "        self.init_lora_layers()# initialise the lora parameters\n",
    "        self.model_vit_lora = vit_model\n",
    "\n",
    "    def init_lora_layers(self) -> None:\n",
    "        \"\"\"\n",
    "        Method to initialise the LoRA layers. A would be initalised using normal distribution and B as 0 i believe\n",
    "        A initialized with small normal values, B to zeros\n",
    "        \"\"\"\n",
    "        for A in self.list_q_As + self.list_v_As:\n",
    "            nn.init.kaiming_uniform_(A.weight, a=math.sqrt(5))\n",
    "            # if you want to use normal distn for initialisation: nn.init.normal_(A.weight, std=1e-3)\n",
    "        for B in self.list_q_Bs + self.list_v_Bs:\n",
    "            nn.init.zeros_(B.weight)\n",
    "\n",
    "        \n",
    "    def save_lora_params(self, filename:str): \n",
    "        \"\"\" \n",
    "\n",
    "        \"\"\"\n",
    "        assert filename.endswith(\".safetensors\"), \"File name is required to have .safetensors extensions\"\n",
    "\n",
    "        # Create dict for safetensors, keys = str, values = tensors\n",
    "        state_dict = {}\n",
    "\n",
    "        # Save lora_layers as a tensor\n",
    "        state_dict['lora_layers'] = torch.tensor(self.lora_layers, dtype=torch.int32)\n",
    "\n",
    "        # Save all LoRA params with keys indicating their index and type\n",
    "        for i, (a_q, b_q, a_v, b_v) in enumerate(zip(self.list_q_As, self.list_q_Bs,self.list_v_As, self.list_v_Bs)):\n",
    "            state_dict[f'q_A_{i}'] = a_q.weight.data\n",
    "            state_dict[f'q_B_{i}'] = b_q.weight.data\n",
    "            state_dict[f'v_A_{i}'] = a_v.weight.data\n",
    "            state_dict[f'v_B_{i}'] = b_v.weight.data\n",
    "\n",
    "        save_file(state_dict, filename)\n",
    "        print(f\"Saved LoRA params and layers to {filename}\")\n",
    "\n",
    "    def load_lora_params(self, filename:str):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        assert filename.endswith(\".safetensors\"), \"File name is required to have .safetensors extensions\"\n",
    "        loaded = load_file(filename)\n",
    "        # Load lora_layers first (convert to list)\n",
    "        loaded_layers = loaded['lora_layers'].tolist()\n",
    "\n",
    "        # If current self.lora_layers differs, you might want to reset or warn\n",
    "        if loaded_layers != self.lora_layers:\n",
    "            print(\"Warning: loaded lora_layers differ from current model's layers. Adjusting...\")\n",
    "            ## maybe here I need to add assertion error so that there is not any major mistake later on\n",
    "            self.lora_layers = loaded_layers\n",
    "            # Optionally: re-initialize LoRA modules for these layers here\n",
    "\n",
    "        # Now load weights into LoRA modules\n",
    "        for i, (a_q, b_q, a_v, b_v) in enumerate(zip(self.list_q_As, self.list_q_Bs, self.list_v_As, self.list_v_Bs)):\n",
    "            a_q.weight.data.copy_(loaded[f'q_A_{i}'])\n",
    "            b_q.weight.data.copy_(loaded[f'q_B_{i}'])\n",
    "            a_v.weight.data.copy_(loaded[f'v_A_{i}'])\n",
    "            b_v.weight.data.copy_(loaded[f'v_B_{i}'])\n",
    "\n",
    "        print(f\"Loaded LoRA params and layers from {filename}\")\n",
    "\n",
    "\n",
    "    def forward(self, x:Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\" \n",
    "        run the LoRA vit\n",
    "        \"\"\"\n",
    "        return self.model_vit_lora(x, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f8ee88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we need to add code to introduce some model for the \n",
    "class SerialLoRALayer(nn.Module):\n",
    "    pass\n",
    "\n",
    "class SerialLoraVit():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c7fb4f",
   "metadata": {},
   "source": [
    "### TO do:\n",
    "\n",
    "1. ~~create a class for the segmentation task (or maybe look into some pre-defined architecture, maybe we might just need to use a simple mlp based architecture?):~~ \n",
    "2. ~~Look into the loss functions one can use for the task here.~~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "475c984d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bce67fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSegHead(nn.Module):\n",
    "    \"\"\" \n",
    "    Custom defined segmentation head. This module takes the patch embeddings from a ViT backbone and\n",
    "    processes them into a full-resolution segmentation map\n",
    "\n",
    "    Arguments:\n",
    "    ----------\n",
    "    hidden_dim : The dimension of the patch embeddings from the ViT (e.g., 768 for ViT-Base).\n",
    "        \n",
    "    num_classes : The number of output segmentation classes (e.g., 21 for PASCAL VOC, 10 for pets dataset).\n",
    "        \n",
    "    patch_size : The size of each image patch in pixels (e.g., 16 for 16x16 patches). This also determines how much to upsample the output feature map.\n",
    "\n",
    "    image_size : The height/width of the input image in pixels (assumes square images). Used to calculate how many patches per spatial dimension.\n",
    "    \"\"\"\n",
    "    def __init__(self, hidden_dim:int, num_classes:int, patch_size:int, image_size:int):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Store the patch size (e.g., 16 for 16x16 patches)\n",
    "        self.patch_size = patch_size\n",
    "        \n",
    "        # Calculate the number of patches per spatial dimension (assuming square image and patches)\n",
    "        self.num_patch_per_dim = image_size // patch_size\n",
    "\n",
    "        # First conv layer: reduces channels from hidden_dim to half, with 3x3 kernel for local spatial context\n",
    "        self.conv1 = nn.Conv2d(hidden_dim, hidden_dim // 2, kernel_size=3, padding=1)\n",
    "        \n",
    "        # ReLU activation after conv1 for non-linearity\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Final conv layer: maps features to the number of classes with 1x1 convolution (pixel-wise classification)\n",
    "        self.conv2 = nn.Conv2d(hidden_dim // 2, num_classes, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):  # x shape: (B, N, D)\n",
    "        \"\"\"\n",
    "        Input x's shape: (B,N,D)\n",
    "        where B= Extract batch size, N= number of patches, and D= embedding dimension.\n",
    "        \"\"\"\n",
    "        # Extract batch size (B), number of patches (N), and embedding dimension (D)\n",
    "        B, N, D = x.shape \n",
    "        \n",
    "        # Calculate height and width of patch grid (assume square grid)\n",
    "        H = W = self.num_patch_per_dim\n",
    "        # Note: N= HXW\n",
    "        \n",
    "        # current shape is : (B,N,D)\n",
    "        # Rearrange tensor to (B, D, H, W) so it can be processed by Conv2d layers\n",
    "        # 1- permute swaps the dimensions so channels come before spatial dims\n",
    "            # -- Swaps dimensions 1 and 2 -> new shape is: (B,D,N) \n",
    "        # 2- reshape organizes tokens into 2D spatial layout: (B,D,N)--> (B,D,H,W)\n",
    "        x = x.permute(0, 2, 1).reshape(B, D, H, W)\n",
    "        print(f\"shape after permutation and rehaping: {x.shape}\")\n",
    "        \n",
    "        # Apply first convolution and ReLU activation to learn local spatial features\n",
    "        x = self.relu(self.conv1(x))\n",
    "        print(f\"shape after first convolution: {x.shape}\")\n",
    "        \n",
    "        # Apply final 1x1 convolution to produce per-class scores for each spatial location\n",
    "        x = self.conv2(x)\n",
    "        print(f\"shape after second convolution: {x.shape}\")\n",
    "        \n",
    "        # Upsample output to match original image resolution\n",
    "        # scale_factor = patch size because each patch corresponds to patch_size x patch_size pixels\n",
    "        x = F.interpolate(x, scale_factor=self.patch_size, mode='bilinear', align_corners=False)\n",
    "        print(f\"shape after interpolation: {x.shape}\")\n",
    "        \n",
    "        # Return segmentation logits of shape (B, num_classes, H_img, W_img)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9097c40",
   "metadata": {},
   "source": [
    "define a vit-seg model: (club together vit-model-feature-extraction with Custom Seg Head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00d63f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the line below while creating a new file:\n",
    "#   -from custom_seg_head import CustomSegHead  # Ensure this path is correct\n",
    "\n",
    "## ?? Questions for me: \n",
    "## 1. Understand this part of the code: Remove classification head if present\n",
    "## 2. Remove CLS token → (B, N, D): (CHECKED)yes we need to do this:\n",
    "\n",
    "class SegViT(nn.Module):\n",
    "    \"\"\"\n",
    "    Wraps a ViT model and applies a custom segmentation head to the output.\n",
    "    Converts patch embeddings into full-resolution segmentation masks.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,vit_model: nn.Module,\n",
    "                    image_size: int,\n",
    "                    patch_size: int,\n",
    "                    dim: int,\n",
    "                    n_classes: int,\n",
    "                    ) -> None:\n",
    "        super().__init__()\n",
    "        \n",
    "        self.vit = vit_model\n",
    "\n",
    "        # Remove classification head if present\n",
    "        if hasattr(self.vit, \"fc\"):\n",
    "            del self.vit.fc\n",
    "        elif hasattr(self.vit, \"lora_vit\") and hasattr(self.vit.lora_vit, \"fc\"):\n",
    "            del self.vit.lora_vit.fc\n",
    "\n",
    "        # Use custom segmentation head\n",
    "        self.seg_head = CustomSegHead(\n",
    "            hidden_dim=dim,\n",
    "            num_classes=n_classes,\n",
    "            patch_size=patch_size,\n",
    "            image_size=image_size\n",
    "        )\n",
    "    # note for me: maybe have an argument stating whether wanna remove the extra class token or not (if it is already removed)\n",
    "    def forward(self, x): # shape of x: (B,N+1,D)\n",
    "        \"\"\"\n",
    "        Forward pass:\n",
    "        1. Get ViT patch embeddings (B, N+1, D)\n",
    "        2. Remove class token (CLS) → (B, N, D)\n",
    "        3. Feed to custom segmentation head\n",
    "        \"\"\"\n",
    "\n",
    "        print(f\"[Input] shape: {x.shape}, type: {type(x)}\")\n",
    "        x = self.vit(x)  # (B, N+1, D)\n",
    "        print(f\"[After vit(x)] type: {type(x)}\")\n",
    "\n",
    "        # get the last state\n",
    "        x= x.last_hidden_state # (B,N+1,D)\n",
    "        print(f\"[After .last_hidden_state] shape: {x.shape}, type: {type(x)}\")\n",
    "\n",
    "        #x = x[:, :-1, :]  # Remove CLS token → (B, N, D)\n",
    "        # I think it should be this:\n",
    "        x = x[:,1:,:] # because it is the first token which is\n",
    "        print(f\"x.shape after removing the CLS: {x.shape}\")\n",
    "        x = self.seg_head(x)  # (B, num_classes, H, W)\n",
    "        print(f\"x.shape after segmentation head: {x.shape}\")\n",
    "        # \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9866d98a",
   "metadata": {},
   "source": [
    "task for today (31-july-2025):\n",
    "\n",
    "0. Investigate the dataset\n",
    "1. ~~Check whether the models are working as intended before training.~~\n",
    "2. ~~Define the loss functions.~~\n",
    "\n",
    "    * ~~Cross entropy loss and weighted CE loss (-log loss).~~\n",
    "        * `nn.CrossEntropyLoss`\n",
    "        * In pytorch, input should be unnormalised logits.\n",
    "\n",
    "    * ~~Dice loss.~~ \n",
    "    * ~~Log-Cosh Dice loss (finite and continuous gradients):~~\n",
    "        * Original paper: https://arxiv.org/pdf/2006.14822\n",
    "        * Review paper on loss: https://arxiv.org/html/2312.05391v1/#S3\n",
    "3. Define the trainer class. \n",
    "4. test whether the code is working as intended for training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f308992",
   "metadata": {},
   "source": [
    "Defining the loss functions for image segmentation tasks\n",
    "\n",
    "1. Done\n",
    "2. need to do some checks in the code below to confirm my implementation is indeed correct. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2a7ebfd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34f3c7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy():\n",
    "    pass\n",
    "\n",
    "def dice_loss(logits, targets, num_classes, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Computes multi-class Dice loss.\n",
    "\n",
    "    Args:\n",
    "        logits: Tensor of shape (N, C, H, W) — raw model outputs\n",
    "        targets: Tensor of shape (N, H, W) — ground truth class indices\n",
    "        num_classes: int — number of classes\n",
    "        epsilon: float — smoothing factor to avoid division by zero\n",
    "\n",
    "    Returns:\n",
    "        Scalar Dice loss\n",
    "    \"\"\"\n",
    "    # Convert targets to one-hot encoding\n",
    "    targets_one_hot = F.one_hot(targets, num_classes).permute(0, 3, 1, 2).float()  # (N, C, H, W)\n",
    "\n",
    "    # Apply softmax to logits \n",
    "    probs = F.softmax(logits, dim=1)  # (N, C, H, W)\n",
    "\n",
    "    # Calculate per-class Dice score\n",
    "    ### ?? Check whether this work as intended or not ??\n",
    "    dims = (0, 2, 3)  # sum over batch, height, width\n",
    "    intersection = torch.sum(probs * targets_one_hot, dims)\n",
    "    cardinality = torch.sum(probs + targets_one_hot, dims)\n",
    "\n",
    "    dice_per_class = (2. * intersection + epsilon) / (cardinality + epsilon)\n",
    "    \n",
    "    #?? I need to test this mean because I am not sure whether the dimension wise it is correct or not ??\n",
    "    # Handling binary vs multi-class\n",
    "    if num_classes == 2:\n",
    "        dice_loss_value = 1. - dice_per_class[1]  # Only use foreground class\n",
    "    else:\n",
    "        dice_loss_value = 1. - dice_per_class.mean()  # Average over all classes\n",
    "        \n",
    "    return dice_loss_value\n",
    "\n",
    "\n",
    "def log_cosh_dice_loss(logits, targets, num_classes, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Computes log-cosh of the multi-class Dice loss.\n",
    "\n",
    "    Args:\n",
    "        logits: Tensor of shape (N, C, H, W)\n",
    "        targets: Tensor of shape (N, H, W)\n",
    "        num_classes: int\n",
    "        epsilon: float\n",
    "\n",
    "    Returns:\n",
    "        Scalar log-cosh Dice loss\n",
    "    \"\"\"\n",
    "    dice = dice_loss(logits, targets, num_classes, epsilon)\n",
    "    return torch.log(torch.cosh(dice))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab83e8f",
   "metadata": {},
   "source": [
    "### checking the model code before training\n",
    "\n",
    "1. ~~Lora_Vit class:~~\n",
    "2. ~~Custom segmentation head~~\n",
    "3. SegViT model (I havent checked it fully yet. Need to do that)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "40434355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dim is: 768\n"
     ]
    }
   ],
   "source": [
    "## checking implementation of LoraVIT class\n",
    "check_loravit = LoraVit(vit_model=model, r=4, alpha = 16, lora_layers=[0,1,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89ac0c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 does not have lora params whereas 3 has:\n",
    "# check_loravit.model_vit_lora.encoder.layer[2], check_loravit.model_vit_lora.encoder.layer[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635a13e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(check_loravit.model_vit_lora)\n",
    "# note that right now the patch size is 16x16 (since kernel size is 16)\n",
    "# your image size needs to be the multiple of 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8cc9f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing the output:\n",
      "['__annotations__', '__class__', '__class_getitem__', '__contains__', '__dataclass_fields__', '__dataclass_params__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__getstate__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__ior__', '__iter__', '__le__', '__len__', '__lt__', '__match_args__', '__module__', '__ne__', '__new__', '__or__', '__post_init__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__ror__', '__setattr__', '__setitem__', '__sizeof__', '__str__', '__subclasshook__', 'attentions', 'clear', 'copy', 'fromkeys', 'get', 'hidden_states', 'items', 'keys', 'last_hidden_state', 'move_to_end', 'pooler_output', 'pop', 'popitem', 'setdefault', 'to_tuple', 'update', 'values']\n",
      "odict_keys(['last_hidden_state', 'pooler_output', 'hidden_states', 'attentions'])\n",
      "torch.Size([1, 197, 768]) torch.Size([1, 768])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, 3, 224, 224)\n",
    "with torch.no_grad():\n",
    "    output=check_loravit(x,output_hidden_states=True, output_attentions=True)\n",
    "print(\"printing the output:\");print(dir(output))\n",
    "print(output.keys())# this one is better\n",
    "print(output.last_hidden_state.shape, output.pooler_output.shape)\n",
    "# note: the output.last_hidden_state.shape is [1,197,768] and not [1,196,768] implying that the output also contains the ClS token. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498cc773",
   "metadata": {},
   "outputs": [],
   "source": [
    "#output.hidden_states, output.attentions ## output of each transformer block, attention weights inside each attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0da11185",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape after permutation and rehaping: torch.Size([25, 768, 14, 14])\n",
      "shape after first convolution: torch.Size([25, 384, 14, 14])\n",
      "shape after second convolution: torch.Size([25, 37, 14, 14])\n",
      "shape after interpolation: torch.Size([25, 37, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "## checking custom segmentation head\n",
    "check_seg_head_model = CustomSegHead(hidden_dim=768, num_classes=37, patch_size=16, image_size=224)\n",
    "x=torch.randn(25,196,768) # I was getting error when I had 197 instead of 196\n",
    "with torch.no_grad():\n",
    "    output = check_seg_head_model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b811720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Input] shape: torch.Size([25, 3, 224, 224]), type: <class 'torch.Tensor'>\n",
      "[After vit(x)] type: <class 'transformers.modeling_outputs.BaseModelOutputWithPooling'>\n",
      "[After .last_hidden_state] shape: torch.Size([25, 197, 768]), type: <class 'torch.Tensor'>\n",
      "x.shape after removing the CLS: torch.Size([25, 196, 768])\n",
      "shape after permutation and rehaping: torch.Size([25, 768, 14, 14])\n",
      "shape after first convolution: torch.Size([25, 384, 14, 14])\n",
      "shape after second convolution: torch.Size([25, 3, 14, 14])\n",
      "shape after interpolation: torch.Size([25, 3, 224, 224])\n",
      "x.shape after segmentation head: torch.Size([25, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "## checking the vit_based_segmentation model\n",
    "x=torch.randn(25,3,224,224)\n",
    "vit_seg_wrapper = SegViT(vit_model=model,image_size=224, patch_size=16, dim=768, n_classes=3)\n",
    "with torch.no_grad():\n",
    "    output =  vit_seg_wrapper(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5249aee8",
   "metadata": {},
   "source": [
    "verdict:  fixed the minor issue in the segmentation wrapper class and it seems to be working fine :)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0a75fd",
   "metadata": {},
   "source": [
    "Download the relevant dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be4161a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2a88e39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the experiment and get it moving :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "70c77176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more advanced adapter based method\n",
    "# what are the other methods that we can have for this work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
