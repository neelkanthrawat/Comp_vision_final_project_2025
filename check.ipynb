{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca8e79f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "347a3e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel, ViTImageProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "551a5e23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## load the pre-trained ViT-model (86 Mil)\n",
    "model_name = 'google/vit-base-patch16-224'\n",
    "\n",
    "# \n",
    "image_processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "model = ViTModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a7fa7b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters: 86,389,248\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "# 86 million model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d759253a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a6670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## I need to start working with the VIT model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0053d7",
   "metadata": {},
   "source": [
    "## Things to do:\n",
    "\n",
    "## Stage 0. We need to come up with some framework of the project and what exactly are we going to do\n",
    "\n",
    "## Stage 1. Set up a Transformer based segmentation model using ViT+LoRA\n",
    "0. Understand and play around with the models\n",
    "1. Load the pre-trained model\n",
    "    1.If required, we might have to switch to timm\n",
    "2. Define the class for LoRA:\n",
    "    2.1: Either can be set up using `peft` library\n",
    "    2.2: Build a custom LoRA module using Pytorch.\n",
    "3. Apply LoRA to attention layers\n",
    "4. Define (code) and add a segmentation head\n",
    "    1. Simple MLP or some more complicated architecture? We need to look into it.\n",
    "5. Training Set-Up\n",
    "    5.1. Loss for the segmentation task\n",
    "    5.2 optimizer\n",
    "6. Training:\n",
    "    6.1 typical LoRA rank (r): 4 or 8 -  a good balance for fine tuning\n",
    "    6.2 How many parameters to freeze\n",
    "\n",
    "Obtain some simple acceptable results for this. \n",
    "\n",
    "## Stage 2. Try  1 more different versions of LoRA for the same task:\n",
    "0. Serial LoRA for the ViT (recent paper)\n",
    "1. Other  DoRA, etc. (check that review paper for other version)\n",
    "\n",
    "## Stage 3: Try 1 other different fine tuning strategy, maybe some other adapter based approach/ IA3 etc. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76a7a17",
   "metadata": {},
   "source": [
    "LoRA implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5031d8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import math\n",
    "from safetensors.torch import save_file, load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c95e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "### implement the LoRAlayer\n",
    "class LoraLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    This class implements the LoRA layer\n",
    "    wt_linear: Weight (which would be left frozen)\n",
    "    A,B: Lower matrices which constitute delta W\n",
    "    rank_lora: Rank of A and B matrices\n",
    "    alpha: some weighing factor\n",
    "    \"\"\"\n",
    "    def __init__(self, wt: nn.Module, A: nn.Module, B: nn.Module, rank_lora: int, alpha: int):\n",
    "        super().__init__()\n",
    "        self.wt = wt\n",
    "        self.A, self.B = A, B\n",
    "        self.rank = rank_lora\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self,x):\n",
    "\n",
    "        x=self.wt(x) + (self.alpha / self.rank) * self.B(self.A(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150ae4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## implement the LoRA VIT\n",
    "### ? Things to check ?\n",
    "## ? vit_model.transformer.blocks\n",
    "## ? vit_model.parameters()\n",
    "class LoraVit(nn.Module):\n",
    "    \"\"\" \n",
    "    This class is to introduce LoRA layer to the model.\n",
    "    vit_model: pre-trained vit model\n",
    "    r: rank\n",
    "    alpha: scaling strength for lora\n",
    "    lora_layers: Layers we want to apply lora to\n",
    "    \"\"\"\n",
    "    def __init__(self, vit_model, r:int, alpha:int, lora_layers=None):\n",
    "        super().__init__()\n",
    "\n",
    "        assert r>0, \"r (rank of lora matrices) must be >0\"\n",
    "        assert alpha>0 , \"alpha >0\"\n",
    "\n",
    "        if lora_layers:\n",
    "            self.lora_layers = lora_layers\n",
    "        else: ## apply lora to all\n",
    "            ## ? here I need to see how will I check the number of transformer blocks\n",
    "            self.lora_layers = list(range(len(vit_model.transformer.blocks)))\n",
    "        \n",
    "        dim = ... # Dimension of the input vector to the transformer\n",
    "        # dim = vit_model.transformer.blocks[0].attn.proj_q.in_features\n",
    "        \n",
    "        # freeze the parameters\n",
    "        ## ? How can we invoke paramters in the vit_model\n",
    "        for param in vit_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        ## for storing the lora parameters\n",
    "        self.list_q_As, self.list_q_Bs = [], []\n",
    "        self.list_v_As, self.list_v_Bs = [], []\n",
    "\n",
    "        # replace the normal q and V with LoRA layers\n",
    "        for layer_i, block_i in enumerate(vit_model.transformer.blocks):\n",
    "            if layer_i not in self.lora_layers:\n",
    "                continue # (next iteration)\n",
    "            w_q_linear = block_i.attn.proj_q\n",
    "            w_v_linear = block_i.attn.proj_v\n",
    "            # Q and V layers' weights\n",
    "\n",
    "            ## do I need to initialise weights here? or should I do it after this loop?\n",
    "            a_linear_q = nn.Linear(dim, r, bias=False)\n",
    "            b_linear_q = nn.Linear(r, dim, bias=False)\n",
    "            a_linear_v = nn.Linear(dim, r, bias=False)\n",
    "            b_linear_v = nn.Linear(r, dim, bias=False)\n",
    "\n",
    "            # Append lora params to the list\n",
    "            self.list_q_As.append(a_linear_q); self.list_q_Bs.append(b_linear_q)\n",
    "            self.list_v_As.append(a_linear_v); self.list_v_Bs.append(b_linear_v) \n",
    "\n",
    "            # replace with LoRA layer\n",
    "            block_i.attn.proj_q = LoraLayer(w_q_linear, a_linear_q, b_linear_q, r, alpha)\n",
    "            block_i.attn.proj_v = LoraLayer(w_v_linear, a_linear_v, b_linear_v, r, alpha)\n",
    "\n",
    "        self.init_lora_layers()# initialise the lora parameters\n",
    "        self.vit_lora = vit_model\n",
    "\n",
    "    def init_lora_layers(self) -> None:\n",
    "        \"\"\"\n",
    "        Method to initialise the LoRA layers. A would be initalised using normal distribution and B as 0 i believe\n",
    "        A initialized with small normal values, B to zeros\n",
    "        \"\"\"\n",
    "        for A in self.list_q_As + self.list_v_As:\n",
    "            nn.init.kaiming_uniform_(A.weight, a=math.sqrt(5))\n",
    "            # if you want to use normal distn for initialisation: nn.init.normal_(A.weight, std=1e-3)\n",
    "        for B in self.list_q_Bs + self.list_v_Bs:\n",
    "            nn.init.zeros_(B.weight)\n",
    "\n",
    "        \n",
    "    def save_lora_params(self, filename:str):\n",
    "        \"\"\" \n",
    "\n",
    "        \"\"\"\n",
    "        # Create dict for safetensors, keys = str, values = tensors\n",
    "        state_dict = {}\n",
    "\n",
    "        # Save lora_layers as a tensor\n",
    "        state_dict['lora_layers'] = torch.tensor(self.lora_layers, dtype=torch.int32)\n",
    "\n",
    "        # Save all LoRA params with keys indicating their index and type\n",
    "        for i, (a_q, b_q, a_v, b_v) in enumerate(zip(self.list_q_As, self.list_q_Bs,self.list_v_As, self.list_v_Bs)):\n",
    "            state_dict[f'q_A_{i}'] = a_q.weight.data\n",
    "            state_dict[f'q_B_{i}'] = b_q.weight.data\n",
    "            state_dict[f'v_A_{i}'] = a_v.weight.data\n",
    "            state_dict[f'v_B_{i}'] = b_v.weight.data\n",
    "\n",
    "        save_file(state_dict, filename)\n",
    "        print(f\"Saved LoRA params and layers to {filename}\")\n",
    "\n",
    "    def load_lora_params(self, filename:str):\n",
    "        \"\"\" \n",
    "        \n",
    "        \"\"\"\n",
    "        loaded = load_file(filename)\n",
    "        # Load lora_layers first (convert to list)\n",
    "        loaded_layers = loaded['lora_layers'].tolist()\n",
    "\n",
    "        # If current self.lora_layers differs, you might want to reset or warn\n",
    "        if loaded_layers != self.lora_layers:\n",
    "            print(\"Warning: loaded lora_layers differ from current model's layers. Adjusting...\")\n",
    "            self.lora_layers = loaded_layers\n",
    "            # Optionally: re-initialize LoRA modules for these layers here\n",
    "\n",
    "        # Now load weights into LoRA modules\n",
    "        for i, (a_q, b_q, a_v, b_v) in enumerate(zip(self.list_q_As, self.list_q_Bs, self.list_v_As, self.list_v_Bs)):\n",
    "            a_q.weight.data.copy_(loaded[f'q_A_{i}'])\n",
    "            b_q.weight.data.copy_(loaded[f'q_B_{i}'])\n",
    "            a_v.weight.data.copy_(loaded[f'v_A_{i}'])\n",
    "            b_v.weight.data.copy_(loaded[f'v_B_{i}'])\n",
    "\n",
    "        print(f\"Loaded LoRA params and layers from {filename}\")\n",
    "\n",
    "\n",
    "    def forward(self, x:Tensor) -> Tensor:\n",
    "        \"\"\" \n",
    "        run the LoRA vit\n",
    "        \"\"\"\n",
    "        return self.vit_lora(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ee88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we need to add code to introduce some model for the \n",
    "class SerialLoRALayer(nn.Module):\n",
    "    pass\n",
    "\n",
    "class SerialLoraVit():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a88e39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the experiment and get it moving :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c77176",
   "metadata": {},
   "outputs": [],
   "source": [
    "# more advanced adapter based method\n",
    "# what are the other methods that we can have for this work"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
