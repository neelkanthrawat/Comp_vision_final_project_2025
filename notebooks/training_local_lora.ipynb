{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aaae003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "# hyperparameter optimization rtd\n",
    "import optuna\n",
    "import wandb\n",
    "\n",
    "# os related\n",
    "import os\n",
    "\n",
    "# file handling\n",
    "\n",
    "# segmentation model\n",
    "from localised_lora_vit import LocalizedLoraVit\n",
    "from segmentation_model import SegViT\n",
    "from segmentation_head import CustomSegHead\n",
    "\n",
    "# dataset class\n",
    "from pet_dataset_class import PreprocessedPetDataset\n",
    "\n",
    "# dataloaders\n",
    "from create_dataloaders import get_pet_dataloaders\n",
    "\n",
    "# trainer\n",
    "from trainer import trainer\n",
    "\n",
    "# loss and metrics\n",
    "from loss_and_metrics_seg import * # idk what to import here tbh. Need to look into it\n",
    "\n",
    "# data plotting\n",
    "from data_plotting import plot_random_images_and_trimaps_2\n",
    "\n",
    "# modules for loading the vit model\n",
    "from transformers import ViTModel, ViTImageProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fc19c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa723a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r_block is: 2.0\n"
     ]
    }
   ],
   "source": [
    "rank_A_and_B=16\n",
    "num_blocks=4;num_blocks_per_row=math.sqrt(num_blocks)\n",
    "r_block = rank_A_and_B/(num_blocks*num_blocks_per_row)\n",
    "print(f\"r_block is: {r_block}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671fae27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8d238f87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## load the pre-trained ViT-model (86 Mil)\n",
    "model_name = 'google/vit-base-patch16-224'\n",
    "\n",
    "image_processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "VIT_PRETRAINED = ViTModel.from_pretrained(model_name)\n",
    "\n",
    "# instantiating the lora vit based backbone model\n",
    "lora_vit_base = LocalizedLoraVit(vit_model=VIT_PRETRAINED,\n",
    "                                r_block=int(r_block),\n",
    "                                alpha=32,\n",
    "                                num_blocks_per_row=int(num_blocks_per_row),# per\n",
    "                                )\n",
    "\n",
    "\n",
    "# # instantiate the custom segmentation head\n",
    "# check_seg_head_model = CustomSegHead(hidden_dim=768, num_classes=3, patch_size=16, image_size=224) #  do not need to do that, the SegViT model will do it automatically\n",
    "\n",
    "# instantiate the segmentation model\n",
    "vit_seg_model = SegViT(vit_model=lora_vit_base,image_size=224, patch_size=16\n",
    "                    , dim= 768,\n",
    "                    n_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecb22215",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147456"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_vit_base.num_trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "906a4e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lora_vit import LoraVit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a112b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "image_processor2 = ViTImageProcessor.from_pretrained(model_name)\n",
    "VIT_PRETRAINED2 = ViTModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "482cb39f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTModel(\n",
       "  (embeddings): ViTEmbeddings(\n",
       "    (patch_embeddings): ViTPatchEmbeddings(\n",
       "      (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    )\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "  )\n",
       "  (encoder): ViTEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x ViTLayer(\n",
       "        (attention): ViTAttention(\n",
       "          (attention): ViTSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (output): ViTSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): ViTIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): ViTOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (pooler): ViTPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VIT_PRETRAINED2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecf8f2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#I just noticed this warning: \"Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight'] You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8778521",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using only 40 datapoints out of 7390 total files.\n",
      "train_size: 32, val_size: 4 and test_size: 4\n"
     ]
    }
   ],
   "source": [
    "# get path of image and mask files\n",
    "try:\n",
    "    base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    # __file__ is not defined (e.g. in Jupyter notebook or interactive sessions apparently), fallback to cwd\n",
    "    base_dir = os.getcwd()\n",
    "\n",
    "# Suppose your dataset is in a folder named 'data' inside the project root:\n",
    "data_dir = os.path.join(base_dir, 'data_oxford_iiit')\n",
    "\n",
    "# # Then you can define image and trimap paths relative to that\n",
    "image_folder = os.path.join(data_dir, 'resized_images')\n",
    "trimap_folder = os.path.join(data_dir, 'resized_masks')\n",
    "\n",
    "# create dataloaders\n",
    "train_dl, val_dl, test_dl = get_pet_dataloaders(\n",
    "    image_folder=image_folder,\n",
    "    mask_folder=trimap_folder,\n",
    "    DatasetClass=PreprocessedPetDataset,\n",
    "    all_data=False,\n",
    "    num_datapoints=40, # right now I am just testing whether the things would work or not. I will intentionally overtrain it.\n",
    "    batch_size=34\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5b659ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the input parameters for the trainer\n",
    "trainer_input_params = {\n",
    "    \"model\": vit_seg_model , \n",
    "    \"optimizer\": torch.optim.Adam(params=vit_seg_model.parameters(), lr=5e-4),\n",
    "#     torch.optim.Adam([\n",
    "#     {\"params\": vit_seg_model.backbone_parameters, \"lr\": 1e-5},\n",
    "#     {\"params\": vit_seg_model.head_parameters, \"lr\": 1e-4},\n",
    "# ]),\n",
    "    #\"lr\": 1e-4, # I do not think we need to have this learning rate seperately. need to fix it here :)\n",
    "    \"criterion\": log_cosh_dice_loss,  # or log_cosh_dice_loss, whichever you want to use\n",
    "    \"num_epoch\": 8,\n",
    "    \"dataloaders\": {\n",
    "        \"train\": train_dl,  # replace with your actual DataLoader\n",
    "        \"val\": val_dl       # replace with your actual DataLoader\n",
    "    },\n",
    "    \"use_trap_scheduler\": False,             # or your scheduler instance if you use one\n",
    "    \"device\": \"cpu\",#\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    #\"model_kwargs\": {},            # add any extra forward() kwargs if needed\n",
    "    \"criterion_kwargs\": {\n",
    "        \"num_classes\": 3,\n",
    "        \"epsilon\": 1e-6,\n",
    "        # \"return_metrics\": False    # usually False for training, True for validation if you want metrics ## WE DO NOT NEED THIS HERE\n",
    "    }\n",
    "    #,\"want_backbone_frozen_initially\": False,\n",
    "    #\"freeze_epochs\":2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3d7b9c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the trainer:\n",
    "trainer_seg_model = trainer(**trainer_input_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce925faa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  12%|█▎        | 1/8 [01:37<11:25, 97.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/8] - Train Loss: 0.2279 | Val Loss: 0.1580 | Dice score: 0.42288434505462646 |IOU score: 0.3040440 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  25%|██▌       | 2/8 [03:38<11:08, 111.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/8] - Train Loss: 0.1375 | Val Loss: 0.1140 | Dice score: 0.5133711695671082 |IOU score: 0.4178324 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  38%|███▊      | 3/8 [05:28<09:13, 110.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/8] - Train Loss: 0.1106 | Val Loss: 0.0993 | Dice score: 0.5468373894691467 |IOU score: 0.4645833 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  50%|█████     | 4/8 [07:06<07:02, 105.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/8] - Train Loss: 0.0941 | Val Loss: 0.0917 | Dice score: 0.5650730729103088 |IOU score: 0.4912463 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  62%|██████▎   | 5/8 [08:48<05:13, 104.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/8] - Train Loss: 0.0792 | Val Loss: 0.0882 | Dice score: 0.5736867785453796 |IOU score: 0.5042699 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  75%|███████▌  | 6/8 [10:36<03:31, 105.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/8] - Train Loss: 0.0692 | Val Loss: 0.0865 | Dice score: 0.5781173706054688 |IOU score: 0.5110564 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  88%|████████▊ | 7/8 [12:22<01:45, 105.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/8] - Train Loss: 0.0661 | Val Loss: 0.0850 | Dice score: 0.5817290544509888 |IOU score: 0.5164741 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 8/8 [13:54<00:00, 104.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/8] - Train Loss: 0.0632 | Val Loss: 0.0835 | Dice score: 0.5855134129524231 |IOU score: 0.5219638 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer_seg_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615a87ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# yeah I had a feeling that we might get some error messahe here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
