{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e67884fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "# hyperparameter optimization rtd\n",
    "import optuna\n",
    "import wandb\n",
    "\n",
    "# os related\n",
    "import os\n",
    "\n",
    "# file handling\n",
    "\n",
    "# segmentation model\n",
    "from lora_vit import LoraVit\n",
    "from segmentation_model import SegViT\n",
    "from segmentation_head import CustomSegHead\n",
    "\n",
    "# dataset class\n",
    "from pet_dataset_class import PreprocessedPetDataset\n",
    "\n",
    "# dataloaders\n",
    "from create_dataloaders import get_pet_dataloaders\n",
    "\n",
    "# trainer\n",
    "from trainer import trainer\n",
    "\n",
    "# loss and metrics\n",
    "from loss_and_metrics_seg import * # idk what to import here tbh. Need to look into it\n",
    "\n",
    "# data plotting\n",
    "from data_plotting import plot_random_images_and_trimaps_2\n",
    "\n",
    "# modules for loading the vit model\n",
    "from transformers import ViTModel, ViTImageProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTModel were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "## load the pre-trained ViT-model (86 Mil)\n",
    "model_name = 'google/vit-base-patch16-224'\n",
    "\n",
    "image_processor = ViTImageProcessor.from_pretrained(model_name)\n",
    "VIT_PRETRAINED = ViTModel.from_pretrained(model_name)\n",
    "\n",
    "# instantiating the lora vit based backbone model\n",
    "lora_vit_base = LoraVit(vit_model=VIT_PRETRAINED,\n",
    "                        r=4,alpha = 16, lora_layers = [1,2,3,4])\n",
    "\n",
    "\n",
    "# # instantiate the custom segmentation head\n",
    "# check_seg_head_model = CustomSegHead(hidden_dim=768, num_classes=3, patch_size=16, image_size=224) #  do not need to do that, the SegViT model will do it automatically\n",
    "\n",
    "# instantiate the segmentation model\n",
    "vit_seg_model = SegViT(vit_model=lora_vit_base,image_size=224, patch_size=16\n",
    "                    , dim= 768,\n",
    "                    n_classes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27fe929d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x0000022536E335A0>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_seg_model.backbone_parameters # get backbone parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a6a1ea5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x0000022536E33BC0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vit_seg_model.head_parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using only 10 datapoints out of 7390 total files.\n",
      "train_size: 8, val_size: 1 and test_size: 1\n"
     ]
    }
   ],
   "source": [
    "# get path of image and mask files\n",
    "try:\n",
    "    base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    # __file__ is not defined (e.g. in Jupyter notebook or interactive sessions apparently), fallback to cwd\n",
    "    base_dir = os.getcwd()\n",
    "\n",
    "# Suppose your dataset is in a folder named 'data' inside the project root:\n",
    "data_dir = os.path.join(base_dir, 'data_oxford_iiit')\n",
    "\n",
    "# # Then you can define image and trimap paths relative to that\n",
    "image_folder = os.path.join(data_dir, 'resized_images')\n",
    "trimap_folder = os.path.join(data_dir, 'resized_masks')\n",
    "\n",
    "# create dataloaders\n",
    "train_dl, val_dl, test_dl = get_pet_dataloaders(\n",
    "    image_folder=image_folder,\n",
    "    mask_folder=trimap_folder,\n",
    "    DatasetClass=PreprocessedPetDataset,\n",
    "    all_data=False,\n",
    "    num_datapoints=10, # right now I am just testing whether the things would work or not. I will intentionally overtrain it.\n",
    "    batch_size=5\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the input parameters for the trainer\n",
    "trainer_input_params = {\n",
    "    \"model\": vit_seg_model , \n",
    "    \"optimizer\": torch.optim.Adam([\n",
    "    {\"params\": vit_seg_model.backbone_parameters, \"lr\": 1e-5},\n",
    "    {\"params\": vit_seg_model.head_parameters, \"lr\": 1e-4},\n",
    "]),\n",
    "    #\"lr\": 1e-4, # I do not think we need to have this learning rate seperately. need to fix it here :)\n",
    "    \"criterion\": log_cosh_dice_loss,  # or log_cosh_dice_loss, whichever you want to use\n",
    "    \"num_epoch\": 3,\n",
    "    \"dataloaders\": {\n",
    "        \"train\": train_dl,  # replace with your actual DataLoader\n",
    "        \"val\": val_dl       # replace with your actual DataLoader\n",
    "    },\n",
    "    \"use_trap_scheduler\": False,             # or your scheduler instance if you use one\n",
    "    \"device\": \"cpu\",#\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    #\"model_kwargs\": {},            # add any extra forward() kwargs if needed\n",
    "    \"criterion_kwargs\": {\n",
    "        \"num_classes\": 3,\n",
    "        \"epsilon\": 1e-6,\n",
    "        # \"return_metrics\": False    # usually False for training, True for validation if you want metrics ## WE DO NOT NEED THIS HERE\n",
    "    },\n",
    "    \"want_backbone_frozen_initially\": True,\n",
    "    \"freeze_epochs\":2\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8cd21e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the trainer\n",
    "trainer_seg_model = trainer(**trainer_input_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92942690",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  33%|███▎      | 1/3 [00:43<01:26, 43.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/3] - Train Loss: 0.2264 | Val Loss: 0.2085 | Dice score: 0.3316095769405365 |IOU score: 0.2093176 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:  67%|██████▋   | 2/3 [01:25<00:42, 42.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/3] - Train Loss: 0.1512 | Val Loss: 0.1899 | Dice score: 0.3639777600765228 |IOU score: 0.2384547 \n",
      " Unfreezing backbone at epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs: 100%|██████████| 3/3 [02:34<00:00, 51.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/3] - Train Loss: 0.1212 | Val Loss: 0.1587 | Dice score: 0.4216954708099365 |IOU score: 0.2918825 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer_seg_model.train() # need to look into this thing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea01ec2b",
   "metadata": {},
   "source": [
    "some other test for the updated trainer class:\n",
    "\n",
    "1. test the case when I want to freeze the vit-base for:\n",
    "        ~~1.1 when optimizer have 2 seperate inputs.~~\n",
    "        \n",
    "      ~~1.2 when optimizer have a single input vit_seg_model.parameters()~~"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
