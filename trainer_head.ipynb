{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74f14b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "# os related\n",
    "import os\n",
    "\n",
    "# segmentation head model\n",
    "from segmentation_head import CustomSegHead\n",
    "# dataset class\n",
    "from pet_dataset_class import PreprocessedPetDataset\n",
    "# dataloaders\n",
    "from create_dataloaders import get_pet_dataloaders\n",
    "# loss and metrics\n",
    "from loss_and_metrics_seg import * # idk what to import here tbh. Need to look into it\n",
    "# trainer\n",
    "from trainer import trainer\n",
    "\n",
    "# data plotting\n",
    "from data_plotting import plot_random_images_and_trimaps_2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2655747"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_seg_head_model = #Seg_head_new(...) # define your own similar segmentation head\n",
    "#CustomSegHead(hidden_dim=768, num_classes=3, patch_size=16, image_size=224) \n",
    "check_seg_head_model.num_trainable_params "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98cfc782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get path of image and mask files\n",
    "try:\n",
    "    base_dir = os.path.dirname(os.path.abspath(__file__))\n",
    "except NameError:\n",
    "    # __file__ is not defined (e.g. in Jupyter notebook or interactive sessions apparently), fallback to cwd\n",
    "    base_dir = os.getcwd()\n",
    "\n",
    "# Suppose your dataset is in a folder named 'data' inside the project root:\n",
    "data_dir = os.path.join(base_dir, 'data_oxford_iiit')\n",
    "\n",
    "# # Then you can define image and trimap paths relative to that\n",
    "image_folder = os.path.join(data_dir, 'resized_images')\n",
    "trimap_folder = os.path.join(data_dir, 'resized_masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb1d8e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Using only 80 datapoints out of 7390 total files.\n",
      "train_size: 64, val_size: 8 and test_size: 8\n"
     ]
    }
   ],
   "source": [
    "# create dataloaders\n",
    "train_dl, val_dl, test_dl = get_pet_dataloaders(\n",
    "    image_folder=image_folder,\n",
    "    mask_folder=trimap_folder,\n",
    "    DatasetClass=PreprocessedPetDataset,\n",
    "    all_data=False,\n",
    "    num_datapoints=80, # right now I am just testing whether the things would work or not. I will intentionally overtrain it.\n",
    "    batch_size=24\n",
    ") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "34a4e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the input parameters for the trainer\n",
    "trainer_segmentation_params = {\n",
    "    \"model\": check_seg_head_model , \n",
    "    \"optimizer\": torch.optim.Adam(check_seg_head_model.parameters(), lr=1e-4),\n",
    "    \"lr\": 1e-4,\n",
    "    \"criterion\": log_cosh_dice_loss,  # or log_cosh_dice_loss, whichever you want to use\n",
    "    \"num_epoch\": 10,\n",
    "    \"dataloaders\": {\n",
    "        \"train\": train_dl,  # replace with your actual DataLoader\n",
    "        \"val\": val_dl       # replace with your actual DataLoader\n",
    "    },\n",
    "    \"use_trap_scheduler\": False,             # or your scheduler instance if you use one\n",
    "    \"device\": \"cpu\",#\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    #\"model_kwargs\": {},            # add any extra forward() kwargs if needed\n",
    "    \"criterion_kwargs\": {\n",
    "        \"num_classes\": 3,\n",
    "        \"epsilon\": 1e-6,\n",
    "        # \"return_metrics\": False    # usually False for training, True for validation if you want metrics ## WE DO NOT NEED THIS HERE\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "780695b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "img_tensor shape: torch.Size([24, 3, 224, 224])\n",
      "remapped_tensor shape: torch.Size([24, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "for idx, data in enumerate(train_dl):\n",
    "    datas = data[0]\n",
    "    remapped = data[1]\n",
    "    print(\"img_tensor shape:\", datas.shape)\n",
    "    print(\"remapped_tensor shape:\",remapped.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e715ab44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the trainer\n",
    "trainer_seg_head =  trainer(**trainer_segmentation_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1926c150",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/10 [00:33<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtrainer_seg_head\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Desktop\\master_scientific_computing\\fourth_semester\\Comp_vision_final_project_2025\\trainer.py:89\u001b[39m, in \u001b[36mtrainer.train\u001b[39m\u001b[34m(self, k)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, k=\u001b[32m1\u001b[39m):\n\u001b[32m     87\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_epoch), desc=\u001b[33m\"\u001b[39m\u001b[33mEpochs\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     88\u001b[39m         \u001b[38;5;66;03m# train and validation step for i'th epoch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m         avg_epoch_train_loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m         avg_epoch_val_loss, avg_epoch_val_dice, avg_epoch_val_iou = \u001b[38;5;28mself\u001b[39m.val_epoch()\u001b[38;5;66;03m# unpack all three values returned by val_epoch\u001b[39;00m\n\u001b[32m     92\u001b[39m         \u001b[38;5;66;03m# accumulate losses\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Desktop\\master_scientific_computing\\fourth_semester\\Comp_vision_final_project_2025\\trainer.py:117\u001b[39m, in \u001b[36mtrainer.train_epoch\u001b[39m\u001b[34m(self, epoch)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28mself\u001b[39m.optimizer.zero_grad()\n\u001b[32m    116\u001b[39m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m output_minibatch = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminibatch_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;66;03m# calculate loss\u001b[39;00m\n\u001b[32m    119\u001b[39m loss_ith_epoch_minibatch = \u001b[38;5;28mself\u001b[39m.criterion(output_minibatch, truth, **\u001b[38;5;28mself\u001b[39m.criterion_kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NEELKANTH RAWAT\\.conda\\envs\\vit\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\NEELKANTH RAWAT\\.conda\\envs\\vit\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Desktop\\master_scientific_computing\\fourth_semester\\Comp_vision_final_project_2025\\segmentation_head.py:66\u001b[39m, in \u001b[36mCustomSegHead.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     62\u001b[39m \u001b[33;03mInput x's shape: (B,N,D)\u001b[39;00m\n\u001b[32m     63\u001b[39m \u001b[33;03mwhere B= Extract batch size, N= number of patches, and D= embedding dimension.\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# Extract batch size (B), number of patches (N), and embedding dimension (D)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m B, N, D = x.shape \n\u001b[32m     68\u001b[39m \u001b[38;5;66;03m# Calculate height and width of patch grid (assume square grid)\u001b[39;00m\n\u001b[32m     69\u001b[39m H = W = \u001b[38;5;28mself\u001b[39m.num_patch_per_dim\n",
      "\u001b[31mValueError\u001b[39m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "trainer_seg_head.train() # so this is inde"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
